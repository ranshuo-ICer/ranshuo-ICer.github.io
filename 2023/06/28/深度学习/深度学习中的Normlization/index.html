<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="RSIC">
    
    <title>
        
            深度学习中的Normlization |
        
        RSIC&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/logo.jpg">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"ranshuo-icer.github.io","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"toc":{"enable":true,"number":true,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066cc","logo":"/images/logo.jpg","favicon":"/images/logo.jpg","avatar":"/images/logo.jpg","font_size":null,"font_family":null,"hover":{"shadow":true,"scale":true},"first_screen":{"enable":true,"header_transparent":false,"background_img":"/images/bg.svg","description":"岁月本长，而忙者自促；天地本宽，而鄙者自隘；风花雪月本闲，而扰攘者自冗","font_color":null,"hitokoto":false},"scroll":{"progress_bar":true,"percent":false}},"local_search":{"enable":true,"preload":true},"code_copy":{},"code_block":{"tools":{"enable":true,"style":"default"},"highlight_theme":"default"},"side_tools":{},"pjax":{"enable":true},"lazyload":{"enable":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.8"},"waline":{"server_url":null,"reaction":false,"version":2}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"enable":true,"wordcount":true,"min2read":true},"img_align":"left","copyright_info":true},"version":"3.6.1"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/logo.jpg">
                </a>
            
            <a class="logo-title" href="/">
               RSIC&#39;s Blog
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/categories"
                            >
                                分类
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/categories">分类</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            <div class="article-title">
                <span class="title-hover-animation">深度学习中的Normlization</span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/logo.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">RSIC</span>
                            
                                <span class="author-label">Lv2</span>
                            
                        </div>
                        <div class="meta-info">
                            
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2023-06-28 21:05:33</span>
        <span class="mobile">2023-06-28 21:05</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2023-06-28 21:58:28</span>
    </span>
    
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fas fa-file-word"></i>&nbsp;<span>2.3k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fas fa-clock"></i>&nbsp;<span>14 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content keep-markdown-body">
                

                <hr>
<p>转载自<a class="link"   target="_blank" rel="noopener" href="https://www.pinecone.io/learn/batch-layer-normalization/" >Batch and Layer Normalization | Pinecone<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<p>Recent advances in deep learning research have revolutionized fields like medical imaging, machine vision, and <a class="link"   target="_blank" rel="noopener" href="https://www.pinecone.io/learn/nlp/" >natural language processing<i class="fas fa-external-link-alt"></i></a>. However, it’s still challenging for data scientists to choose the optimal model architecture and to tune hyperparameters for best results.</p>
<p>Even with the optimal model architecture, how the model is trained can make the difference between a phenomenal success or a scorching failure.</p>
<p>For example, take <a class="link"   target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-2/#init" >weight initialization<i class="fas fa-external-link-alt"></i></a>: In the process of training a neural network, we initialize the weights which are then updated as the training proceeds. For a certain random initialization, the outputs from one or more of the intermediate layers can be abnormally large. This leads to instability in the training process, which means the network will not learn anything useful during training.</p>
<p>Batch and layer normalization are two strategies for training neural networks faster, without having to be overly cautious with initialization and other <a class="link"   target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-2/#reg" >regularization<i class="fas fa-external-link-alt"></i></a> techniques.</p>
<p>In this tutorial, we’ll go over the need for normalizing inputs to the neural network and then proceed to learn the techniques of batch and layer normalization.</p>
<p>Let’s get started!</p>
<h2 id="Why-Should-You-Normalize-Inputs-in-a-Neural-Network"><a href="#Why-Should-You-Normalize-Inputs-in-a-Neural-Network" class="headerlink" title="Why Should You Normalize Inputs in a Neural Network?"></a>Why Should You Normalize Inputs in a Neural Network?</h2><p>When you train a neural network on a dataset, the numeric input features could take on values in potentially different ranges. For example, if you’re working with a dataset of student loans with the age of the student and the tuition as two input features, the two values are on totally <em>different</em> scales. While the age of a student will have a median value in the range 18 to 25 years, the tuition could take on values in the range $20K - $50K for a given academic year.</p>
<p>If you proceed to train your model on such datasets with input features on different scales, you’ll notice that the neural network takes significantly longer to train because the gradient descent algorithm takes longer to converge when the input features are not all on the same scale. Additionally, such high values can also propagate through the layers of the network leading to the accumulation of large error gradients that make the training process unstable, called the problem of  <em>exploding gradients</em> .</p>
<p>To overcome the above-mentioned issues of longer training time and instability, you should consider preprocessing your input data ahead of training. Preprocessing techniques such as normalization and standardization transform the input data to be on the same scale.</p>
<h3 id="Normalization-vs-Standardization"><a href="#Normalization-vs-Standardization" class="headerlink" title="Normalization vs Standardization"></a>Normalization vs Standardization</h3><p>Normalization works by mapping all values of a feature to be in the range [0,1] using the transformation:</p>
<p>$$<br>x_{norm}&#x3D;\frac{x-x_{min}}{x_{max}-x_{min}}<br>$$</p>
<p>Suppose a particular input feature <code>x</code> has values in the range <code>[x_min, x_max]</code>. When <code>x</code> is equal to <code>x_min</code>, <code>x_norm</code> is equal to 0 and when <code>x</code> is equal to <code>x_max</code>, <code>x_norm</code> is equal to 1. So for all values of <code>x</code> between <code>x_min</code> and <code>x_max</code>, <code>x_norm</code> maps to a value between 0 and 1.</p>
<p>Standardization, on the other hand, transforms the input values such that they follow a distribution with zero mean and unit variance. Mathematically, the transformation on the data points in a distrbution with mean μ and standard deviation σ is given by:</p>
<p>$$<br>x_{std}&#x3D;\frac{x-\mu}{\sigma}<br>$$</p>
<p>In practice, this process of <em>standardization</em> is also referred to as <em>normalization</em> (not to be confused with the normalization process discussed above). As part of the preprocessing step, you can add a layer that applies this transform to the input features so that they all have a similar distribution. In Keras, you can add a <a class="link"   target="_blank" rel="noopener" href="https://keras.io/api/layers/preprocessing_layers/numerical/normalization/" >normalization layer<i class="fas fa-external-link-alt"></i></a> that applies this transform to the input features.</p>
<h2 id="Need-for-Batch-Normalization"><a href="#Need-for-Batch-Normalization" class="headerlink" title="Need for Batch Normalization"></a>Need for Batch Normalization</h2><p>In the previous section, we learned how we can normalize the input to the neural network in order to speed up training. If you look at the neural network architecture, the input layer is not the only input layer. For a network with hidden layers, the output of layer <code>k-1</code> serves as the input to layer <code>k</code>. If the inputs to a particular layer change drastically, we can again run into the problem of unstable gradients.</p>
<p>When working with large datasets, you’ll split the dataset into multiple batches and run the mini-batch gradient descent. The <a class="link"   target="_blank" rel="noopener" href="https://d2l.ai/chapter_optimization/minibatch-sgd.html" >mini-batch gradient descent<i class="fas fa-external-link-alt"></i></a> algorithm optimizes the parameters of the neural network by batchwise processing of the dataset, one batch at a time.</p>
<p>It’s also possible that the input distribution at a particular layer keeps changing across batches. The seminal paper titled <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167" >Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<i class="fas fa-external-link-alt"></i></a> by Sergey Ioffe and Christian Szegedy refers to this change in distribution of the input to a particular layer across batches as  <em>internal covariate shift</em> . For instance, if the distribution of data at the input of layer K keeps changing across batches, the network will take longer to train.</p>
<p><em>But why does this hamper the training process?</em></p>
<p>For each batch in the input dataset, the mini-batch gradient descent algorithm runs its updates. It updates the weights and biases (parameters) of the neural network so as to fit to the distribution seen at the input to the specific layer for the current batch.</p>
<p>Now that the network has learned to fit to the current distribution, if the distribution changes substantially for the next batch, it now has to update the parameters to fit to the new distribution. This slows down the training process.</p>
<p>However, if we transpose the idea of <em>normalizing the inputs</em> to the <em>hidden</em> layers in the network, we can potentially overcome the limitations imposed by exploding activations and fluctuating distributions at the layer’s input. Batch normalization helps us achieve this, one mini-batch at a time, to accelerate the training process.</p>
<h2 id="What-is-Batch-Normalization"><a href="#What-is-Batch-Normalization" class="headerlink" title="What is Batch Normalization?"></a>What is Batch Normalization?</h2><p>For any hidden layer <code>h</code>, we pass the inputs through a non-linear activation to get the output. For every neuron (activation) in a particular layer, we can force the pre-activations to have zero mean and unit standard deviation. This can be achieved by subtracting the mean from each of the input features across the mini-batch and dividing by the standard deviation.</p>
<p>Following the output of the layer <code>k-1</code>, we can add a layer that performs this normalization operation across the mini-batch so that the pre-activations at layer <code>k</code> are unit Gaussians. The figure below illustrates this.</p>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://d33wubrfki0l68.cloudfront.net/770321b325d7e6537e0ed94a207c5e881f8ecb88/661b8/images/batch-normalization-layer.png"
                      alt="Neural Network with Batch Normalization Layer"
                ></p>
<pre><code>Section of a Neural Network with Batch Normalization Layer (Image by the author)
</code></pre>
<p>As an example, let’s consider a mini-batch with 3 input samples, each input vector being four features long. Here’s a simple illustration of how the mean and standard deviation are computed in this case. Once we compute the mean and standard deviation, we can subtract the mean and divide by the standard deviation.</p>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://d33wubrfki0l68.cloudfront.net/5863322b42dcdf4b45ffef4de43f6ef0385db477/e6251/images/batch-normalization-example.png"
                      alt="Batch Normalization Example"
                ></p>
<pre><code>How Batch Normalization Works - An Example (Image by the author)
</code></pre>
<p>However, forcing all the pre-activations to be zero and unit standard deviation across all batches can be too restrictive. It may be the case that the fluctuant distributions are necessary for the network to learn certain classes better.</p>
<p>To address this, batch normalization introduces two parameters: a scaling factor <code>gamma</code> (γ) and an offset <code>beta</code> (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of <code>gamma</code> and <code>beta</code> for each mini-batch. The <code>gamma</code> and <code>beta</code> are learnable such that it’s possible to go back from the normalized pre-activations to the actual distributions that the pre-activations follow.</p>
<p>Putting it all together, we have the following steps for batch normalization. If <code>x(k)</code> is the pre-activation corresponding to the k-th neuron in a layer, we denote it by <code>x</code> to simplify notation.</p>
<p>$$<br>\begin{align}<br>\mu_{b}&#x3D;\frac{1}{B}\sum_{i&#x3D;1}^Bx_i\<br>\sigma^2_b&#x3D;\frac{1}{B}\sum_{i&#x3D;1}^B(x_i-\mu_b)^2\<br>\hat{x_i}&#x3D;\frac{x_i-\mu_b}{\sqrt{\sigma^2_b}}\<br>or\ \hat{x_i}&#x3D;\frac{x_i-\mu_b}{\sqrt{\sigma^2_b+\epsilon}}\<br>y_i&#x3D;BN(x_i)&#x3D;\gamma\cdot x_i+\beta<br>\end{align}<br>$$</p>
<p>Adding $\epsilon$  helps when  $\sigma^2_b$ is small</p>
<h3 id="Limitations-of-Batch-Normalization"><a href="#Limitations-of-Batch-Normalization" class="headerlink" title="Limitations of Batch Normalization"></a>Limitations of Batch Normalization</h3><p>Two limitations of batch normalization can arise:</p>
<ul>
<li>In batch normalization, we use the  <em>batch statistics</em> : the mean and standard deviation corresponding to the current mini-batch. However, when the batch size is small, the sample mean and sample standard deviation are not representative enough of the actual distribution and the network cannot learn anything meaningful.</li>
<li>As batch normalization depends on batch statistics for normalization, it is less suited for sequence models. This is because, in sequence models, we may have sequences of potentially different lengths and smaller batch sizes corresponding to longer sequences.</li>
</ul>
<p>Later, we’ll examine layer normalization, another technique that can be used for sequence models. For convolutional neural networks (ConvNets), batch normalization is still recommended for faster training.</p>
<h3 id="How-to-Add-a-Batch-Normalization-Layer-in-Keras"><a href="#How-to-Add-a-Batch-Normalization-Layer-in-Keras" class="headerlink" title="How to Add a Batch Normalization Layer in Keras"></a>How to Add a Batch Normalization Layer in Keras</h3><p>Keras provides a <code>BatchNormalization</code> class that lets you add a batch normalization layer wherever needed in the model architecture. For a complete review of the different parameters you can use to customize the batch normalization layer, refer to the <a class="link"   target="_blank" rel="noopener" href="https://keras.io/api/layers/normalization_layers/batch_normalization/" >Keras docs for BatchNormalization<i class="fas fa-external-link-alt"></i></a>.</p>
<p>The code snippet below shows how you can add batch normalization layers to an arbitrary sequential model in Keras. You can choose to selectively apply batch normalization to specific layers in the network.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, BatchNormalization</span><br><span class="line"></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">10</span>, input_shape=(<span class="number">1</span>,<span class="number">4</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    <span class="comment"># add batchnorm layer after activations in the previous layer</span></span><br><span class="line">    BatchNormalization(axis=<span class="number">1</span>),</span><br><span class="line">    <span class="comment"># pre-activations at the dense layer below are Gaussians</span></span><br><span class="line">    Dense(units=<span class="number">16</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    BatchNormalization(axis=<span class="number">1</span>),</span><br><span class="line">    Dense(units=<span class="number">4</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>Copy</p>
<p>It’s important to understand how batch normalization works under the hood during training and testing. During training, batch normalization computes the mean and standard deviation corresponding to the mini-batch.</p>
<p>However, at test time (inference time), we may not necessarily have a batch to compute the batch mean and variance. To overcome this limitation, the model works by maintaining a <a class="link"   target="_blank" rel="noopener" href="https://mathworld.wolfram.com/MovingAverage.html" >moving average<i class="fas fa-external-link-alt"></i></a> of the mean and variance at training time, called the moving mean and moving variance. These values are accumulated across batches at training time and used as mean and variance at inference time.</p>
<h2 id="What-is-Layer-Normalization"><a href="#What-is-Layer-Normalization" class="headerlink" title="What is Layer Normalization?"></a>What is Layer Normalization?</h2><p><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450" >Layer Normalization<i class="fas fa-external-link-alt"></i></a> was proposed by researchers Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. In layer normalization, all neurons in a particular layer effectively have the same distribution across all features for a given input.</p>
<p>For example, if each input has <code>d</code> features, it’s a d-dimensional vector. If there are <code>B</code> elements in a batch, the normalization is done along the length of the d-dimensional vector and not across the batch of size <code>B</code>.</p>
<p>Normalizing <em>across all features</em> but for each of the inputs to a specific layer removes the dependence on batches. This makes layer normalization well suited for sequence models such as <a class="link"   target="_blank" rel="noopener" href="https://www.pinecone.io/learn/sentence-embeddings/" >transformers<i class="fas fa-external-link-alt"></i></a> and <a class="link"   target="_blank" rel="noopener" href="https://www.ibm.com/cloud/learn/recurrent-neural-networks" >recurrent neural networks (RNNs)<i class="fas fa-external-link-alt"></i></a> that were popular in the pre-transformer era.</p>
<p>Here’s an example showing the computation of the mean and variance for layer normalization. We consider the example of a mini-batch containing three input samples, each with four features.</p>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://d33wubrfki0l68.cloudfront.net/c8f1f7a886548f82234f8a3b06faeecfbb88c657/42d49/images/layer-normalization.png"
                      alt="Layer Normalization"
                >How Layer Normalization Works - An Example (Image by the author)</p>
<p>$$<br>\begin{align}<br>\mu_{l}&#x3D;\frac{1}{d}\sum_{i&#x3D;1}^dx_i\<br>\sigma^2_l&#x3D;\frac{1}{d}\sum_{i&#x3D;1}^d(x_i-\mu_l)^2\<br>\hat{x_i}&#x3D;\frac{x_i-\mu_l}{\sqrt{\sigma^2_l}}\<br>or\ \hat{x_i}&#x3D;\frac{x_i-\mu_l}{\sqrt{\sigma^2_l+\epsilon}}\<br>y_i&#x3D;BN(x_i)&#x3D;\gamma\cdot x_i+\beta<br>\end{align}<br>$$</p>
<p>Adding $\epsilon$  helps when  $\sigma^2_b$ is small</p>
<p>From these steps, we see that they’re similar to the steps we had in batch normalization. However, instead of the batch statistics, we use the mean and variance corresponding to specific input to the neurons in a particular layer, say <code>k</code>. This is equivalent to normalizing the output vector from the layer <code>k-1</code>.</p>
<h3 id="How-to-Add-a-Layer-Normalization-in-Keras"><a href="#How-to-Add-a-Layer-Normalization-in-Keras" class="headerlink" title="How to Add a Layer Normalization in Keras"></a>How to Add a Layer Normalization in Keras</h3><p>Similar to batch normalization, Keras also provides a <code>LayerNormalization</code> class that you can use to add layer normalization to the inputs of specific layers. The code cell below shows how you can add <a class="link"   target="_blank" rel="noopener" href="https://keras.io/api/layers/normalization_layers/layer_normalization/" >layer normalization<i class="fas fa-external-link-alt"></i></a> in a simple sequential model. The parameter <code>axis</code> specifies the axis along which the normalization should be done.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, LayerNormalization</span><br><span class="line"></span><br><span class="line">model = Sequential([</span><br><span class="line">    Dense(units=<span class="number">16</span>, input_shape=(<span class="number">1</span>,<span class="number">10</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    LayerNormalization(axis=<span class="number">1</span>),</span><br><span class="line">    Dense(units=<span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">    LayerNormalization(axis=<span class="number">1</span>),</span><br><span class="line">    Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>Copy</p>
<p>To understand how layer normalization is used in transformers, consider reading this TensorFlow tutorial on <a class="link"   target="_blank" rel="noopener" href="https://www.tensorflow.org/text/tutorials/transformer" >transformer models for language understanding<i class="fas fa-external-link-alt"></i></a>.</p>
<h2 id="Batch-Normalization-vs-Layer-Normalization"><a href="#Batch-Normalization-vs-Layer-Normalization" class="headerlink" title="Batch Normalization vs Layer Normalization"></a>Batch Normalization vs Layer Normalization</h2><p>So far, we learned how batch and layer normalization work. Let’s summarize the key differences between the two techniques.</p>
<ul>
<li>Batch normalization normalizes each feature independently across the mini-batch. Layer normalization normalizes each of the inputs in the batch independently across all features.</li>
<li>As batch normalization is dependent on batch size, it’s not effective for small batch sizes. Layer normalization is independent of the batch size, so it can be applied to batches with smaller sizes as well.</li>
<li>Batch normalization requires different processing at training and inference times. As layer normalization is done along the length of input to a specific layer, the same set of operations can be used at both training and inference times.</li>
</ul>
<h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>In this tutorial, you learned the basics of and differences between batch and layer normalization techniques and how to implement them in Keras.</p>
<p>Over the past several years, batch normalization and layer normalization have emerged as the go-to normalization techniques in computer vision and natural language processing, respectively. In certain computer vision tasks, <a class="link"   target="_blank" rel="noopener" href="https://www.tensorflow.org/addons/tutorials/layers_normalizations" >group and instance normalization<i class="fas fa-external-link-alt"></i></a> are also used. For further reading, consider checking out the recommended resources in the section below. Happy learning!</p>

            </div>

            
                <div class="post-copyright-info">
                    
<div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        <li class="post-title">
            <span class="type">本文标题</span>：<span class="content">深度学习中的Normlization</span>
        </li>
        <li class="post-author">
            <span class="type">本文作者</span>：<span class="content">RSIC</span>
        </li>
        <li class="post-time">
            <span class="type">创建时间</span>：<span class="content">2023-06-28 21:05:33</span>
        </li>
        <li class="post-link">
            <span class="type">本文链接</span>：<span class="content">2023/06/28/深度学习/深度学习中的Normlization/</span>
        </li>
        <li class="post-license">
            <span class="type">版权声明</span>：<span class="content">本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！</span>
        </li>
    </ul>
    <div class="copy-copyright-info flex-center tooltip" data-content="复制版权信息" data-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                               rel="next"
                               href="/2023/06/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CrossEntropyLoss,%20NLLLoss%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/"
                            >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">Pytorch损失函数CrossEntropyLoss和NLLLoss</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                                <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                            </a>
                        </div>
                    
                </div>
            

            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Should-You-Normalize-Inputs-in-a-Neural-Network"><span class="nav-number">1.</span> <span class="nav-text">Why Should You Normalize Inputs in a Neural Network?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalization-vs-Standardization"><span class="nav-number">1.1.</span> <span class="nav-text">Normalization vs Standardization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Need-for-Batch-Normalization"><span class="nav-number">2.</span> <span class="nav-text">Need for Batch Normalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Batch-Normalization"><span class="nav-number">3.</span> <span class="nav-text">What is Batch Normalization?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitations-of-Batch-Normalization"><span class="nav-number">3.1.</span> <span class="nav-text">Limitations of Batch Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-Add-a-Batch-Normalization-Layer-in-Keras"><span class="nav-number">3.2.</span> <span class="nav-text">How to Add a Batch Normalization Layer in Keras</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Layer-Normalization"><span class="nav-number">4.</span> <span class="nav-text">What is Layer Normalization?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-Add-a-Layer-Normalization-in-Keras"><span class="nav-number">4.1.</span> <span class="nav-text">How to Add a Layer Normalization in Keras</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization-vs-Layer-Normalization"><span class="nav-number">5.</span> <span class="nav-text">Batch Normalization vs Layer Normalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Final-Thoughts"><span class="nav-number">6.</span> <span class="nav-text">Final Thoughts</span></a></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2020</span> -
            
            2023
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">RSIC</a>
            
        </div>
        
            <script async data-pjax
                    src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    访问人数&nbsp;<span id="busuanzi_value_site_uv"></span>&ensp;
                
                
                    总访问量&nbsp;<span id="busuanzi_value_site_pv"></span>
                
            </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script>




    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/local-search.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script>


<div class="post-scripts pjax">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/post-helper.js"></script>
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/anime.min.js"></script>
        
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/toc.js"></script>
        
    
</div>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
